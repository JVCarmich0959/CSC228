{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrVF3I7AsBR0"
      },
      "outputs": [],
      "source": [
        "# First Steps with Python's NLTK Library\n",
        "# Uses Libraries:  NLTK\n",
        "# Runtime:  Google CoLab (cpu)\n",
        "# Traditional Machine Learning Approach\n",
        "# \n",
        "# Owner:  Lorrie Tomek\n",
        "# \n",
        "# Data: \n",
        "# variety of corpora downloaded at runtime from NLTK\n",
        "#\n",
        "# Reference:  Real Python \n",
        "# URL: https://realpython.com/python-nltk-sentiment-analysis/#customizing-nltks-sentiment-analysis\n",
        "# The tutorial is freely available on the internet.  The content of this Google Colab notebook has been modified for teaching purposes.\n",
        "# (Verified December 2022)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Data Analytics using NLTK\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this tutorial, you’ll be ready to:\n",
        "\n",
        "* Split and filter text data in preparation for analysis\n",
        "*   Analyze word frequency\n",
        "*   Find concordance and collocations using different methods"
      ],
      "metadata": {
        "id": "T9Sf3GWrsyMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is NLTK?\n",
        "\n",
        "The NLTK library has many utilities for manipulation and analysis of linguistic (natural language) data.  These include text classifiers that can be used for many different types of classification, including sentiment analysis.  .\n",
        "\n",
        "To use NLTK, we need to install it using pip, and import the libraries we need."
      ],
      "metadata": {
        "id": "IcVgt207tzxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use pip to install the nltk library\n",
        "! pip install nltk"
      ],
      "metadata": {
        "id": "lAET2H_Ztql7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c0f071-1b03-4d3e-82dd-ba5151a30a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries we will use\n",
        "import nltk\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "TVzNzYSFuK9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading NLTK Resources\n",
        "\n",
        "NLTK has a large collection of resources that can be freely downloaded.  Here is a list of some of them: \n",
        "\n",
        "*   names: A list of common English names compiled by Mark Kantrowitz\n",
        "*   stopwords: A list of really common words, like articles, pronouns, prepositions, and conjunctions\n",
        "*   state_union: A sample of transcribed State of the Union addresses by different US presidents, compiled by Kathleen Ahrens\n",
        "*   movie_reviews: Two thousand movie reviews categorized by Bo Pang and Lillian Lee\n",
        "*   twitter samples:  A sample of twitter data\n",
        "*   averaged_perceptron_tagger: A data model that NLTK uses to categorize words into their part of speech\n",
        "*   vader_lexicon: A scored list of words and jargon that NLTK references when performing sentiment analysis, created by C.J. Hutto and Eric Gilbert\n",
        "*   punkt: A data model created by Jan Strunk that NLTK uses to split full texts into word lists\n",
        "*   shakespeare:  A sample of text from Shakespeare\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eUAvuNrOvBpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download some of the many nltk resources\n",
        "nltk.download([\n",
        "  \"names\",\n",
        "  \"stopwords\",\n",
        "  \"state_union\",\n",
        "  \"twitter_samples\",\n",
        "  \"movie_reviews\",\n",
        "  \"averaged_perceptron_tagger\",\n",
        "  \"vader_lexicon\",\n",
        "  \"punkt\",\n",
        "  \"shakespeare\"\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTZR61pxuZnA",
        "outputId": "a18f1801-3aa5-4971-91b5-bfc57dc8851e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/shakespeare.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is a corpus?  \n",
        "A corpus is a large collection of related text samples. In the list above, we downloaded several corpra.  Corpra is the plural form of corpus.  We downloaded:  state of the union, twitter samples, movie reviews as well as shakespeare. In the context of NLTK, corpora are compiled with features for natural language processing (NLP), such as categories and numerical scores for particular features."
      ],
      "metadata": {
        "id": "O-bsIilJxYRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration\n",
        "\n",
        "NLTK includes a number of functions that will help you meaningfully analyze text, even before doing anything more complex like machine learning. Many of NLTK’s utilities are helpful in preparing your data for more advanced analysis.\n",
        "\n",
        "Let's explore data using from our downloaded corpra to learn about frequency distribution, cordordance, and collections. "
      ],
      "metadata": {
        "id": "b68tjUkewXN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start with the State of the Union corpus\n",
        "# Let's look at the words in the state_union corpus\n",
        "# We use the .words() method to get the words, and build a list of words that\n",
        "# are alphabetic (so we don't include \"words\" that are punctuation symbols)\n",
        "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
        "# You can just type:  nltk.corpus.state_union_words() to see what you\n",
        "# would get if you did not filter for stopwords\n",
        "\n",
        "# Here we show the first 100 words:\n",
        "pprint(words[:100], width=79, compact=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlVxWS-IwS1B",
        "outputId": "5408a979-e937-4a89-ad88-c29c72147776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRESIDENT', 'HARRY', 'S', 'TRUMAN', 'S', 'ADDRESS', 'BEFORE', 'A', 'JOINT',\n",
            " 'SESSION', 'OF', 'THE', 'CONGRESS', 'April', 'Mr', 'Speaker', 'Mr',\n",
            " 'President', 'Members', 'of', 'the', 'Congress', 'It', 'is', 'with', 'a',\n",
            " 'heavy', 'heart', 'that', 'I', 'stand', 'before', 'you', 'my', 'friends',\n",
            " 'and', 'colleagues', 'in', 'the', 'Congress', 'of', 'the', 'United', 'States',\n",
            " 'Only', 'yesterday', 'we', 'laid', 'to', 'rest', 'the', 'mortal', 'remains',\n",
            " 'of', 'our', 'beloved', 'President', 'Franklin', 'Delano', 'Roosevelt', 'At',\n",
            " 'a', 'time', 'like', 'this', 'words', 'are', 'inadequate', 'The', 'most',\n",
            " 'eloquent', 'tribute', 'would', 'be', 'a', 'reverent', 'silence', 'Yet', 'in',\n",
            " 'this', 'decisive', 'hour', 'when', 'world', 'events', 'are', 'moving', 'so',\n",
            " 'rapidly', 'our', 'silence', 'might', 'be', 'misunderstood', 'and', 'might',\n",
            " 'give', 'comfort', 'to', 'our']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords\n",
        "\n",
        "In the list of words shown above, you will see many common words like \"a\", \"the\", \"and\".  In NLP processing, it is often useful to filter out these common words called stopwords, because they do not add a lot to the meaning of the text.\n",
        "\n",
        "We use the NLTK stopwords corpus to remove the stopwords.  You will notice that the stopwords in that corpus are all lower case.  \n",
        "\n",
        "When we want to filter out words from our state_union word list we use str.lower() so that we compare lower case words to lower case words.  If we don't do that, we end up with upper case or mixed case words like \"The\" and \"THE\" not being recognized and filtered as stop words. "
      ],
      "metadata": {
        "id": "rS1kxCDA2lOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of stopwords from the stopwords corpus\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "pprint(stopwords[:100], width=79, compact=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdknHTIb2T8J",
        "outputId": "e5455d74-1956-4e15-ff68-f737907c7337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",\n",
            " \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he',\n",
            " 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it',\n",
            " \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
            " 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
            " 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
            " 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',\n",
            " 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with',\n",
            " 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
            " 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
            " 'over', 'under', 'again', 'further', 'then', 'once']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the stopwords \n",
        "words = [w for w in words if w.lower() not in stopwords]\n",
        "# notice in the filtered list below, there are no stopwords\n",
        "pprint(words[:100], width=79, compact=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKzBsWvq3Oei",
        "outputId": "a3324470-a45c-402b-d823-0eef28a650f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRESIDENT', 'HARRY', 'TRUMAN', 'ADDRESS', 'JOINT', 'SESSION', 'CONGRESS',\n",
            " 'April', 'Mr', 'Speaker', 'Mr', 'President', 'Members', 'Congress', 'heavy',\n",
            " 'heart', 'stand', 'friends', 'colleagues', 'Congress', 'United', 'States',\n",
            " 'yesterday', 'laid', 'rest', 'mortal', 'remains', 'beloved', 'President',\n",
            " 'Franklin', 'Delano', 'Roosevelt', 'time', 'like', 'words', 'inadequate',\n",
            " 'eloquent', 'tribute', 'would', 'reverent', 'silence', 'Yet', 'decisive',\n",
            " 'hour', 'world', 'events', 'moving', 'rapidly', 'silence', 'might',\n",
            " 'misunderstood', 'might', 'give', 'comfort', 'enemies', 'infinite', 'wisdom',\n",
            " 'Almighty', 'God', 'seen', 'fit', 'take', 'us', 'great', 'man', 'loved',\n",
            " 'beloved', 'humanity', 'man', 'could', 'possibly', 'fill', 'tremendous',\n",
            " 'void', 'left', 'passing', 'noble', 'soul', 'words', 'ease', 'aching',\n",
            " 'hearts', 'untold', 'millions', 'every', 'race', 'creed', 'color', 'world',\n",
            " 'knows', 'lost', 'heroic', 'champion', 'justice', 'freedom', 'Tragic', 'fate',\n",
            " 'thrust', 'upon', 'us']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "For the purpose of this example, Python's string methods are sufficient to allow us to analyze the words in the corpus.  \n",
        "\n",
        "Looking ahead, there is a rich topic called **tokenization** which splits text into individual items called **tokens**.  We may have **word tokenization**, which splits text into words which are seperated by spaces.  We may want to remove punctuation during tokenization, or we may want punctuation symbols to be tokens.  We may want the word \"don't\" to be one token: \"don't\" or multiple tokens: \"do\" \"n't\" or multiple tokens:  \"don\" \"'\" \"t\".  \n",
        "\n",
        "Using nltk, you can build your own custom corpus.  To do so, you would need to learn more about tokenization.  \n",
        "\n",
        "You can learn more about NLTK corpus at this URL: \n",
        "https://www.nltk.org/howto/corpus.html\n",
        "\n",
        "For now, let's take a multi-line string that we hard-code in a variable called text, and use nltk's word_tokenize() method to get list of words."
      ],
      "metadata": {
        "id": "En4zzvmw5dMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here's some arbitrary multi-line text\n",
        "example_text = \"\"\"\n",
        "For some quick analysis, creating a corpus could be overkill.\n",
        "If all you need is a word list,\n",
        "there are simpler ways to achieve that goal.\n",
        "\"\"\"\n",
        "\n",
        "# use nltk's word_tokenize() method get the tokens\n",
        "example_text_tokens = nltk.word_tokenize(example_text)\n",
        "\n",
        "# display the tokens for our example text\n",
        "pprint(example_text_tokens, width=79, compact=True)\n",
        "\n",
        "# OOPS.... see that there is a token that is a \".\"\n",
        "# Let's filter out the non-alphabetic tokens\n",
        "example_text_word_tokens = [w for w in nltk.word_tokenize(example_text) if w.isalpha()]\n",
        "\n",
        "pprint(example_text_word_tokens, width=79, compact=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntsq763p7FYm",
        "outputId": "f406c6d5-3d74-4437-f2ae-ef1192175187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['For', 'some', 'quick', 'analysis', ',', 'creating', 'a', 'corpus', 'could',\n",
            " 'be', 'overkill', '.', 'If', 'all', 'you', 'need', 'is', 'a', 'word', 'list',\n",
            " ',', 'there', 'are', 'simpler', 'ways', 'to', 'achieve', 'that', 'goal', '.']\n",
            "['For', 'some', 'quick', 'analysis', 'creating', 'a', 'corpus', 'could', 'be',\n",
            " 'overkill', 'If', 'all', 'you', 'need', 'is', 'a', 'word', 'list', 'there',\n",
            " 'are', 'simpler', 'ways', 'to', 'achieve', 'that', 'goal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frequency Distributions\n",
        "\n",
        "Now let's look at frequency distributions. A frequency distribution is a table showing how many times each word appears within a given text. \n",
        "\n",
        "In NLTK, frequency distributions are a specific object type implemented as a distinct class called FreqDist. This class provides useful operations for word frequency analysis.\n",
        "\n",
        "To build a frequency distribution with NLTK, construct the nltk.FreqDist class with a word list:"
      ],
      "metadata": {
        "id": "adSgg2Km814m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's go back to looking at our state_union corpus\n",
        "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
        "\n",
        "# Get the Frequency Disbution using NLTK's FreqDist() method\n",
        "fd = nltk.FreqDist(words)\n",
        "\n",
        "# NLTK's FreqDist() method creates a python object of type nltk.probability.FreqDist\n",
        "# which is similar to a python dictionary, but has additional methods\n",
        "print(type(fd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhaUxvbS9C7H",
        "outputId": "91238a51-eae6-4988-fe07-2fe88d86b530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'nltk.probability.FreqDist'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the most common words (in this case the top 5); notice what is returned is a list of tuples\n",
        "# showing that 'the' is the most common word, and that 'the' appars in the corpus is 19,191 times\n",
        "fd.most_common(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnD7fYhD-F1o",
        "outputId": "fdcca333-07dc-486e-8738-c0eb5bda9d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 19191), ('of', 12854), ('to', 11868), ('and', 11748), ('in', 6936)]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the .tabulate() method to find the same information in a more readable representation\n",
        "fd.tabulate(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-nsgHnv-NeP",
        "outputId": "0fad0b4f-dcf2-45c0-925a-1302a2befcee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  the    of    to   and    in \n",
            "19191 12854 11868 11748  6936 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use the FreqDist to find the frequency of any word\n",
        "fd[\"America\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-wWalHx-1vz",
        "outputId": "98ce71e0-9558-4118-b1d0-c5ab082578e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1076"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Each case of the letters is a different word token.\n",
        "# Here we see that we have 3 cases of AMERICA, all in capitals\n",
        "fd[\"AMERICA\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuHFyrRI_1c1",
        "outputId": "99738b92-f7c3-48f4-ecd1-48ebd02fc64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we can also see that if a word is not the corpus \n",
        "# as \"america\" all lower case is not in the corpus, we do not \n",
        "# get an exception as we would if we looked for the value of \n",
        "# an item in a Python dictionary where the key does not exist\n",
        "fd[\"america\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQlYLe0g_3Sx",
        "outputId": "e618ebe0-c228-4ba1-c8b7-1aba72efa24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can build a Frequency Distribution for words irregardless of\n",
        "# their case (capitals/lower case)\n",
        "words_lower = [w.lower() for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
        "lower_fd = nltk.FreqDist(words_lower)\n",
        "\n",
        "print(lower_fd[\"america\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKZpFN3pAbMR",
        "outputId": "a466176c-bba0-467d-8923-7f7f60332d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concordance\n",
        "\n",
        "In NLP, a concordance is a collection of word locations along with their context. \n",
        "\n",
        "*   Number of times a word appears\n",
        "*   Position where each occurrence appears\n",
        "*   What words surround each occurrence\n",
        "\n",
        "In NLTK, there is a Text object with a method called concordance().  As you will notice below, the case of the word is ignored, and just the words that surround each occurrence are shown."
      ],
      "metadata": {
        "id": "81n05Vv3J9tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = nltk.Text(nltk.corpus.state_union.words())\n",
        "text.concordance(\"america\", lines=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMOUYc4fKiBs",
        "outputId": "69e1056d-774e-4268-8537-5de7cc2675a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 5 of 1079 matches:\n",
            " would want us to do . That is what America will do . So much blood has already\n",
            "ay , the entire world is looking to America for enlightened leadership to peace\n",
            "beyond any shadow of a doubt , that America will continue the fight for freedom\n",
            " to make complete victory certain , America will never become a party to any pl\n",
            "nly in law and in justice . Here in America , we have labored long and hard to \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "concordance_list = text.concordance_list(\"america\", lines=2)\n",
        "for entry in concordance_list:\n",
        "   print(entry.line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q8NfbbiK9v6",
        "outputId": "fd07a94c-9178-4d40-ef73-31534614289d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " would want us to do . That is what America will do . So much blood has already\n",
            "ay , the entire world is looking to America for enlightened leadership to peace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_words = nltk.word_tokenize(\n",
        "\"\"\"Beautiful is better than ugly.\n",
        "Explicit is better than implicit.\n",
        "Simple is better than complex.\"\"\"\n",
        ")\n",
        "example_text= nltk.Text(example_words)\n",
        "# The line below is equivalent to: example_fd = nltk.FreqDist(example_words)\n",
        "example_fd = example_text.vocab()  \n",
        "example_fd.tabulate(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZENwEqALxJj",
        "outputId": "ee074885-fd14-45fe-ba41-a8188a3f4a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    is better   than \n",
            "     3      3      3 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l"
      ],
      "metadata": {
        "id": "7bUil269Nw1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collocations\n",
        "\n",
        "Collocations are series of words that frequently appear together in a given text. In the State of the Union corpus, for example, you’d expect to find the words United and States appearing next to each other often. Those two words appearing together is called a collocation.\n",
        "\n",
        "NLTK can quickly find collocations.  Collocations can be made up of two or more words, typically 2-4 words, which are called:    \n",
        "\n",
        "* Bigrams: Frequent two-word combinations\n",
        "* Trigrams: Frequent three-word combinations\n",
        "* Quadgrams: Frequent four-word combinations\n",
        "\n",
        "The general term is n-grams where n is the number of words collocated.\n"
      ],
      "metadata": {
        "id": "kcFuVIscNCgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The NLTK collocations object has BigramCollocationFinder, TrigramCollocationFinder\n",
        "# and QuadgramCollocationFinder\n",
        "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
        "bifinder = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
        "trifinder = nltk.collocations.TrigramCollocationFinder.from_words(words)\n",
        "quadfinder = nltk.collocations.QuadgramCollocationFinder.from_words(words)"
      ],
      "metadata": {
        "id": "yi0RTrNJNpUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use any of the *gramFinder objects to find the frequency distribution\n",
        "# of the n-grams.  We can therefore find the top-k most common, using the \n",
        "# most_common() with parameter k.\n",
        "print(f\"Most Common 2:  {trifinder.ngram_fd.most_common(2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLtZL2pNOR4z",
        "outputId": "f6a7bd47-733c-4bff-a8cb-93adb12985dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Common 2:  [(('the', 'United', 'States'), 294), (('the', 'American', 'people'), 185)]\n"
          ]
        }
      ]
    }
  ]
}