{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained Word Embeddings\n",
    "\n",
    "# Owner : Jacquelyn Carmichael\n",
    "# Uses Libraries: gensim\n",
    "# Runtime: Jupyter Notebook\n",
    "\n",
    "# Data:\n",
    "\n",
    "# Reference: https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Word Embeddings\n",
    "\n",
    "There are many pretrained Word Embeddings. We can use PreTrained Word Embeddings with Keras. We can use the following pretrained Word Embeddings:\n",
    "\n",
    "* GloVe\n",
    "* Word2Vec\n",
    "* FastText\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessar libraries\n",
    "import pandas as pd \n",
    "\n",
    "# The pandas library is used to read in the data and manipulate it\n",
    "\n",
    "import gensim\n",
    "\n",
    "# The gensim library is used to create the word embeddings\n",
    "\n",
    "import gensim.downloader\n",
    "\n",
    "# The gensim.downloader library is used to download the pre-trained word embeddings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# The numpy library is used to create the word embeddings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The matplotlib library is used to create the word embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300\n",
      "conceptnet-numberbatch-17-06-300\n",
      "word2vec-ruscorpora-300\n",
      "word2vec-google-news-300\n",
      "glove-wiki-gigaword-50\n",
      "glove-wiki-gigaword-100\n",
      "glove-wiki-gigaword-200\n",
      "glove-wiki-gigaword-300\n",
      "glove-twitter-25\n",
      "glove-twitter-50\n"
     ]
    }
   ],
   "source": [
    "for model_name in list(gensim.downloader.info()['models'].keys())[:10]:\n",
    "    print(model_name)\n",
    "\n",
    "# The gensim.downloader.info() function returns a dictionary of information about the pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "google_news_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "\n",
    "# The gensim.downloader.load() function loads the pre-trained word embeddings\n",
    "# The pre-trained word embeddings are stored in the google_news_vectors variable\n",
    "# the word2vec-google-news-300 is the name of the pre-trained word embeddings\n",
    "# it contains 300 dimensional word vectors that were trained on 100 billion words from the Google News datasets\n",
    "# You can use this to find the most similar words to a given word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe Word Embeddings\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "word2vec-google-news-300 is a pretrained word embedding model from Google News dataset. It contains 300-dimensional vectors for 3 million words and phrases. The model can be used to generate word embeddings for out-of-vocabulary words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.8099378347396851),\n",
       " ('dog', 0.760945737361908),\n",
       " ('kitten', 0.7464984655380249),\n",
       " ('feline', 0.7326233983039856),\n",
       " ('beagle', 0.7150583267211914),\n",
       " ('puppy', 0.7075453996658325),\n",
       " ('pup', 0.6934291124343872),\n",
       " ('pet', 0.6891531348228455),\n",
       " ('felines', 0.6755931377410889),\n",
       " ('chihuahua', 0.6709762215614319)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors.most_similar('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apples', 0.720359742641449),\n",
       " ('pear', 0.6450697183609009),\n",
       " ('fruit', 0.6410146355628967),\n",
       " ('berry', 0.6302294731140137),\n",
       " ('pears', 0.6133960485458374),\n",
       " ('strawberry', 0.6058261394500732),\n",
       " ('peach', 0.6025872826576233),\n",
       " ('potato', 0.5960935354232788),\n",
       " ('grape', 0.5935865044593811),\n",
       " ('blueberry', 0.5866668224334717)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets find the most similar words \n",
    "\n",
    "google_news_vectors.most_similar(\"apple\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Word Embeddings\n",
    "\n",
    "FastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices.\n",
    "\n",
    "FastText uses subword information to create vector representations for out-of-vocabulary words. It is also able to provide word vectors for words that are not present in the vocabulary. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.8368596434593201),\n",
       " ('housecat', 0.767471194267273),\n",
       " ('-cat', 0.7602992057800293),\n",
       " ('dog', 0.7502297759056091),\n",
       " ('kitten', 0.7480817437171936),\n",
       " ('feline', 0.7353992462158203),\n",
       " ('super-cat', 0.7305206060409546),\n",
       " ('supercat', 0.7163283824920654),\n",
       " ('pet', 0.7090283632278442),\n",
       " ('moggy', 0.7057286500930786)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FastText Embeddings are a set of pre-trained word embeddings that were trained on Wikipedia and Common Crawl\n",
    "\n",
    "fasttext_vectors = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
    "fasttext_vectors.most_similar('cat')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Word Embeddings\n",
    "\n",
    "Word2Vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. The idea is that words that share contexts in the corpus will be located in close proximity to one another in the space.\n",
    "\n",
    "# Word Embeddings in Keras\n",
    "\n",
    "Keras provides an Embedding layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer.\n",
    "\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "* **input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "\n",
    "* **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "\n",
    "* **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "We can define an Embedding layer as part of a Keras model as follows:\n",
    "\n",
    "```python\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('R._Mazzei_fused', 0.6665399074554443),\n",
       " ('Christian_Audigier_nightclub', 0.6632695198059082),\n",
       " ('copper_alloy_garnets', 0.634365439414978),\n",
       " ('Nelmeus', 0.6274421811103821),\n",
       " ('fiber_fusion_splicing', 0.6229820251464844),\n",
       " ('Plexiglass', 0.5858588814735413),\n",
       " ('slashing_Leonardo_DiCaprio', 0.5850011110305786),\n",
       " ('plexiglass', 0.5823023319244385),\n",
       " ('Plexiglas', 0.5803930759429932),\n",
       " (\"#Q'##_unaudited\", 0.5798528790473938)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors.most_similar(\"glass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Capital of Britain given: (Paris - France) + Britain\n",
      "[('London', 0.7541898488998413)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding Cpital of Britain given the capital of France\n",
    "print(\"Finding Capital of Britain given: (Paris - France) + Britain\")\n",
    "capital = google_news_vectors.most_similar(['Paris', 'Britain'], [\"France\"], topn=1)\n",
    "print(capital)\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
